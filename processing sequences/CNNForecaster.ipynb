{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_type</th>\n",
       "      <th>bus</th>\n",
       "      <th>rail</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-01-01</th>\n",
       "      <td>U</td>\n",
       "      <td>297192</td>\n",
       "      <td>126455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-02</th>\n",
       "      <td>W</td>\n",
       "      <td>780827</td>\n",
       "      <td>501952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-03</th>\n",
       "      <td>W</td>\n",
       "      <td>824923</td>\n",
       "      <td>536432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-04</th>\n",
       "      <td>W</td>\n",
       "      <td>870021</td>\n",
       "      <td>550011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-01-05</th>\n",
       "      <td>W</td>\n",
       "      <td>890426</td>\n",
       "      <td>557917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-27</th>\n",
       "      <td>U</td>\n",
       "      <td>312965</td>\n",
       "      <td>215594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-28</th>\n",
       "      <td>W</td>\n",
       "      <td>611041</td>\n",
       "      <td>389359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-29</th>\n",
       "      <td>W</td>\n",
       "      <td>652674</td>\n",
       "      <td>444706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-30</th>\n",
       "      <td>W</td>\n",
       "      <td>657942</td>\n",
       "      <td>451915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-31</th>\n",
       "      <td>W</td>\n",
       "      <td>605292</td>\n",
       "      <td>425596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8705 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           day_type     bus    rail\n",
       "date                               \n",
       "2001-01-01        U  297192  126455\n",
       "2001-01-02        W  780827  501952\n",
       "2001-01-03        W  824923  536432\n",
       "2001-01-04        W  870021  550011\n",
       "2001-01-05        W  890426  557917\n",
       "...             ...     ...     ...\n",
       "2024-10-27        U  312965  215594\n",
       "2024-10-28        W  611041  389359\n",
       "2024-10-29        W  652674  444706\n",
       "2024-10-30        W  657942  451915\n",
       "2024-10-31        W  605292  425596\n",
       "\n",
       "[8705 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "path = Path(\"C:\\\\Users\\\\manch\\\\OneDrive\\\\Documents\\\\DEV\\\\MachineLearning\\\\datasets\\\\CTA_-_Ridership_-_Daily_Boarding_Totals_20241230.csv\")\n",
    "\n",
    "df = pd.read_csv(path, parse_dates=[\"service_date\"])\n",
    "df.columns = [\"date\", \"day_type\", \"bus\", \"rail\", \"total\"] #shorter names\n",
    "df = df.sort_values(\"date\").set_index(\"date\")\n",
    "df = df.drop(\"total\", axis=1) # no need for total, it's just bus+ rail\n",
    "df = df.drop_duplicates() # remove duplicated months (2011-10and 2014-07)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_windows(dataset, length):\n",
    "    dataset = dataset.window(length, shift=1,drop_remainder=True)\n",
    "    return dataset.flat_map(lambda window_ds:window_ds.batch(length))\n",
    "\n",
    "def to_seq2seq_dataset( series, \n",
    "                        seq_length=56, \n",
    "                        ahead=14,\n",
    "                        target_col=1,\n",
    "                        batch_size=32, \n",
    "                        shuffle=False, \n",
    "                        seed=None):\n",
    "    ds = to_windows(tf.data.Dataset.from_tensor_slices(series),ahead + 1)\n",
    "    ds = to_windows(ds, seq_length).map(lambda S: (S[:, 0], S[:,1:, 1]))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(8 * batch_size, seed=seed)\n",
    "    return ds.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "df_mulvar                   = df[[\"bus\", \"rail\"]] / 1e6 # use both bus & rail series as input\n",
    "df_mulvar[\"next_day_type\"]  = df[\"day_type\"].shift(-1)  # we know tomorrow's type\n",
    "df_mulvar                   = pd.get_dummies(df_mulvar) # one-hot encode the day type Now df_mulvar is a DataFrame with five columns: the bus and rail data,\n",
    "                                                        # plus three columns containing the one-hot encoding of the next day’s type\n",
    "                                                        # (recall that there are three possible day types, W, A, and U).\n",
    "\n",
    "#Split the data into three periods. For training, validation, and testing:\n",
    "mulvar_train = df_mulvar[\"2016-01\":\"2018-12\"]\n",
    "mulvar_valid = df_mulvar[\"2019-01\":\"2019-05\"]\n",
    "mulvar_test  = df_mulvar[\"2019-06\":]\n",
    "\n",
    "# Ensure all columns are numeric, converting booleans to float32\n",
    "mulvar_train = mulvar_train.astype(np.float32)\n",
    "mulvar_valid = mulvar_valid.astype(np.float32)\n",
    "mulvar_test  = mulvar_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv_rnn_model = tf.keras.Sequential(\n",
    "                                        [\n",
    "                                            tf.keras.layers.Conv1D( filters=32, \n",
    "                                                                    kernel_size=4, \n",
    "                                                                    strides=2,\n",
    "                                                                    activation=\"relu\", \n",
    "                                                                    input_shape=[None,5]\n",
    "                                                                    ),\n",
    "                                            tf.keras.layers.GRU(32, return_sequences=True),\n",
    "                                            tf.keras.layers.Dense(14)\n",
    "                                        ]\n",
    "                                    )\n",
    "longer_train = to_seq2seq_dataset(mulvar_train, seq_length=112,shuffle=True, seed=42)\n",
    "longer_valid = to_seq2seq_dataset(mulvar_valid, seq_length=112)\n",
    "\n",
    "downsampled_train = longer_train.map(lambda X, Y: (X, Y[:,3::2]))\n",
    "downsampled_valid = longer_valid.map(lambda X, Y: (X, Y[:,3::2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     31/Unknown \u001b[1m9s\u001b[0m 62ms/step - loss: 0.1402 - mae: 0.4636"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 102ms/step - loss: 0.1384 - mae: 0.4592 - val_loss: 0.0258 - val_mae: 0.1624\n",
      "Epoch 2/500\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0191 - mae: 0.1558 - val_loss: 0.0183 - val_mae: 0.1534\n",
      "Epoch 3/500\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.0168 - mae: 0.1557 - val_loss: 0.0178 - val_mae: 0.1495\n",
      "Epoch 4/500\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 0.0163 - mae: 0.1536 - val_loss: 0.0174 - val_mae: 0.1467\n",
      "Epoch 5/500\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.0158 - mae: 0.1507 - val_loss: 0.0170 - val_mae: 0.1445\n",
      "Epoch 6/500\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - loss: 0.0153 - mae: 0.1480 - val_loss: 0.0165 - val_mae: 0.1422\n",
      "Epoch 7/500\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0149 - mae: 0.1458 - val_loss: 0.0161 - val_mae: 0.1401\n",
      "Epoch 8/500\n",
      "\u001b[1m10/31\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0146 - mae: 0.1458"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\n",
    "\n",
    "conv_rnn_model.compile(loss=tf.keras.losses.Huber(), optimizer=opt,metrics=[\"mae\"])\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=50, restore_best_weights=True)\n",
    "\n",
    "history_conv_rnn_model    =   conv_rnn_model.fit(\n",
    "                                                downsampled_train, \n",
    "                                                validation_data=downsampled_valid,\n",
    "                                                epochs=500,\n",
    "                                                callbacks=[early_stopping_cb]\n",
    "        )\n",
    "# Get MAE values for training and validation\n",
    "mae_values = history_conv_rnn_model.history['mae']  # training MAE\n",
    "val_mae_values = history_conv_rnn_model.history['val_mae']  # validation MAE\n",
    "\n",
    "# Get loss values\n",
    "loss_values = history_conv_rnn_model.history['loss']  # training loss\n",
    "val_loss_values = history_conv_rnn_model.history['val_loss']  # validation loss\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mae_values, label='Training MAE')\n",
    "plt.plot(val_mae_values, label='Validation MAE')\n",
    "plt.title('Model MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Get predictions\n",
    "test_predictions = history_conv_rnn_model.predict(downsampled_valid)\n",
    "\n",
    "# Since we're using a sequence length of 56, our predictions start at index 56\n",
    "# We need to align the predictions with the actual values\n",
    "actual_values = mulvar_valid[\"rail\"][112:].values\n",
    "mae_1 = (pd.DataFrame(test_predictions*1e6) - pd.DataFrame(actual_values*1e6)).abs().mean() \n",
    "print(mae_1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_predictions, label='Predicted values')\n",
    "plt.plot(actual_values, label='Actual Values')\n",
    "plt.title('Model MAE over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
